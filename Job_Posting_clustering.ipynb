{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpMjvfUorxH0",
        "outputId": "79b28c16-606f-4943-bdc1-fdc3e0aa831c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: schedule in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading model: [Errno 2] No such file or directory: 'job_clustering_model.pkl'\n",
            "No existing model found. Will train new model.\n",
            "\n",
            "2025-05-24 10:34:18.830712: Running daily job monitoring...\n",
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Training new clustering model...\n",
            "Optimal clusters: 5 (silhouette score: 0.51)\n",
            "\n",
            "Notification for Data Scientist (user1@example.com):\n",
            "Found 4 new jobs matching your skills:\n",
            "- Machine Learning Physical Design Engineer at Google\n",
            "  Skills: Aartificial intelligence,Algorithms,Data structuring,Design,Machine learning techniques\n",
            "  Posted: 2025-05-24 10:34:26\n",
            "\n",
            "- Senior Product Designer at Observe.AI\n",
            "  Skills: Design,Leadership Skill,Machine learning techniques\n",
            "  Posted: 2025-05-24 10:34:26\n",
            "\n",
            "- Machine Learning Physical Design Engineer at Google\n",
            "  Skills: Aartificial intelligence,Algorithms,Data structuring,Design,Machine learning techniques\n",
            "  Posted: 2025-05-24 10:34:34\n",
            "\n",
            "- Senior Product Designer at Observe.AI\n",
            "  Skills: Design,Leadership Skill,Machine learning techniques\n",
            "  Posted: 2025-05-24 10:34:34\n",
            "\n",
            "\n",
            "Notification for Web Developer (user2@example.com):\n",
            "Found 6 new jobs matching your skills:\n",
            "- Data Scientist Lead - AIML at JPMorgan Chase\n",
            "  Skills: Aartificial intelligence,Data science techniques,Large Language Models - LLMs,Machine learning techniques,Natural Language Processing (NLP),Python Programming\n",
            "  Posted: 2025-05-24 10:34:26\n",
            "\n",
            "- Applied AI ML Director - Machine Learning at JPMorgan Chase\n",
            "  Skills: Aartificial intelligence,AWS,Azure,Google Cloud Platform (GCP),Kubernetes-K8s,Large Language Models - LLMs,Machine learning techniques\n",
            "  Posted: 2025-05-24 10:34:26\n",
            "\n",
            "- Manager - Machine Learning at Observe.AI\n",
            "  Skills: Aartificial intelligence,Large Language Models - LLMs,Machine learning techniques,Natural Language Processing (NLP)\n",
            "  Posted: 2025-05-24 10:34:26\n",
            "\n",
            "- Data Scientist Lead - AIML at JPMorgan Chase\n",
            "  Skills: Aartificial intelligence,Data science techniques,Large Language Models - LLMs,Machine learning techniques,Natural Language Processing (NLP),Python Programming\n",
            "  Posted: 2025-05-24 10:34:34\n",
            "\n",
            "- Applied AI ML Director - Machine Learning at JPMorgan Chase\n",
            "  Skills: Aartificial intelligence,AWS,Azure,Google Cloud Platform (GCP),Kubernetes-K8s,Large Language Models - LLMs,Machine learning techniques\n",
            "  Posted: 2025-05-24 10:34:34\n",
            "\n",
            "- Manager - Machine Learning at Observe.AI\n",
            "  Skills: Aartificial intelligence,Large Language Models - LLMs,Machine learning techniques,Natural Language Processing (NLP)\n",
            "  Posted: 2025-05-24 10:34:34\n",
            "\n",
            "\n",
            "Sample of clustered jobs:\n",
            "                                               title      company  \\\n",
            "0   Principal Product Manager - Growth, Poe (Remote)  Quora, Inc.   \n",
            "1          Machine Learning Physical Design Engineer       Google   \n",
            "2  Staff Software Engineer - Monetization, Poe (R...  Quora, Inc.   \n",
            "3  Staff Backend Engineer - Bot Creator Ecosystem...  Quora, Inc.   \n",
            "4  Senior Backend Engineer - Bot Creator Ecosyste...  Quora, Inc.   \n",
            "\n",
            "                                              skills  cluster  \n",
            "0  Aartificial intelligence,Data Analytics,Data s...        2  \n",
            "1  Aartificial intelligence,Algorithms,Data struc...        0  \n",
            "2  Aartificial intelligence,Analytical and Proble...        1  \n",
            "3  Aartificial intelligence,API,Data science tech...        1  \n",
            "4  Aartificial intelligence,API,Data science tech...        1  \n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install beautifulsoup4 requests pandas numpy scikit-learn nltk schedule\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import schedule\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    BASE_URL = \"https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}\"\n",
        "    USER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
        "    REQUEST_DELAY = 2  # seconds between requests\n",
        "    CLUSTER_RANGE = range(2, 6)  # Range of clusters to try\n",
        "    MODEL_FILE = 'job_clustering_model.pkl'\n",
        "    DATA_FILE = 'job_postings.csv'\n",
        "\n",
        "    # Sample user profiles (modify as needed)\n",
        "    USER_PROFILES = [\n",
        "        {\n",
        "            'name': 'Data Scientist',\n",
        "            'skills': 'python, machine learning, statistics, data analysis',\n",
        "            'email': 'user1@example.com'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Web Developer',\n",
        "            'skills': 'javascript, html, css, react',\n",
        "            'email': 'user2@example.com'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "# Text Preprocessor\n",
        "class TextPreprocessor:\n",
        "    @staticmethod\n",
        "    def preprocess(text):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        words = text.split()\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "# Job Scraper (based on your code)\n",
        "class JobScraper:\n",
        "    def __init__(self):\n",
        "        self.headers = {'User-Agent': Config.USER_AGENT}\n",
        "\n",
        "    def scrape_jobs(self, keyword=\"data science\", pages=2):\n",
        "        jobs_list = []\n",
        "\n",
        "        for page in range(1, pages + 1):\n",
        "            url = Config.BASE_URL.format(\n",
        "                page=page,\n",
        "                query=keyword.replace(' ', '%20')\n",
        "            )\n",
        "\n",
        "            try:\n",
        "                print(f\"Scraping page {page}...\")\n",
        "                response = requests.get(url, headers=self.headers, timeout=10)\n",
        "                response.raise_for_status()\n",
        "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "                job_blocks = soup.find_all(\"div\", class_=\"ads-details\")\n",
        "\n",
        "                for job in job_blocks:\n",
        "                    try:\n",
        "                        title = job.find(\"h4\").get_text(strip=True)\n",
        "                        company = job.find(\"a\", href=lambda x: x and \"Employer-Profile\" in x).get_text(strip=True)\n",
        "                        location = job.find(\"p\").get_text(strip=True)\n",
        "                        experience = job.find(\"p\", class_=\"emp-exp\").get_text(strip=True)\n",
        "                        key_skills_tag = job.find(\"span\", string=\"Key Skills\")\n",
        "                        skills = key_skills_tag.find_next(\"p\").get_text(strip=True) if key_skills_tag else \"\"\n",
        "                        summary_tag = job.find(\"span\", string=\"Summary\")\n",
        "                        summary = summary_tag.find_next(\"p\").get_text(strip=True) if summary_tag else \"\"\n",
        "\n",
        "                        jobs_list.append({\n",
        "                            \"title\": title,\n",
        "                            \"company\": company,\n",
        "                            \"location\": location,\n",
        "                            \"experience\": experience,\n",
        "                            \"description\": summary,\n",
        "                            \"skills\": skills,\n",
        "                            \"scraped_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                        })\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error parsing job: {e}\")\n",
        "                        continue\n",
        "\n",
        "                time.sleep(Config.REQUEST_DELAY)\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping page {page}: {e}\")\n",
        "                continue\n",
        "\n",
        "        return pd.DataFrame(jobs_list)\n",
        "\n",
        "# Job Clustering System\n",
        "class JobClusterer:\n",
        "    def __init__(self):\n",
        "        self.vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        self.model = None\n",
        "        self.cluster_descriptions = {}\n",
        "\n",
        "    def train_model(self, job_data):\n",
        "        # Preprocess skills\n",
        "        job_data['processed_skills'] = job_data['skills'].apply(TextPreprocessor.preprocess)\n",
        "\n",
        "        # Vectorize skills\n",
        "        X = self.vectorizer.fit_transform(job_data['processed_skills'])\n",
        "\n",
        "        # Find optimal number of clusters\n",
        "        best_score = -1\n",
        "        best_n_clusters = 2\n",
        "\n",
        "        for n_clusters in Config.CLUSTER_RANGE:\n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "            cluster_labels = kmeans.fit_predict(X)\n",
        "            score = silhouette_score(X, cluster_labels)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_n_clusters = n_clusters\n",
        "\n",
        "        print(f\"Optimal clusters: {best_n_clusters} (silhouette score: {best_score:.2f})\")\n",
        "\n",
        "        # Train final model\n",
        "        self.model = KMeans(n_clusters=best_n_clusters, random_state=42)\n",
        "        job_data['cluster'] = self.model.fit_predict(X)\n",
        "\n",
        "        # Create cluster descriptions\n",
        "        for cluster_num in range(best_n_clusters):\n",
        "            cluster_data = job_data[job_data['cluster'] == cluster_num]\n",
        "            all_skills = ' '.join(cluster_data['processed_skills']).split()\n",
        "            top_skills = pd.Series(all_skills).value_counts().head(5).index.tolist()\n",
        "            self.cluster_descriptions[cluster_num] = top_skills\n",
        "\n",
        "        return job_data\n",
        "\n",
        "    def classify_job(self, job_posting):\n",
        "        processed_skills = TextPreprocessor.preprocess(job_posting['skills'])\n",
        "        skills_vector = self.vectorizer.transform([processed_skills])\n",
        "        cluster = self.model.predict(skills_vector)[0]\n",
        "        return cluster, self.cluster_descriptions[cluster]\n",
        "\n",
        "    def save_model(self):\n",
        "        with open(Config.MODEL_FILE, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'vectorizer': self.vectorizer,\n",
        "                'model': self.model,\n",
        "                'cluster_descriptions': self.cluster_descriptions\n",
        "            }, f)\n",
        "\n",
        "    def load_model(self):\n",
        "        try:\n",
        "            with open(Config.MODEL_FILE, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "                self.vectorizer = data['vectorizer']\n",
        "                self.model = data['model']\n",
        "                self.cluster_descriptions = data['cluster_descriptions']\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return False\n",
        "\n",
        "# Notification System (simplified for Colab)\n",
        "class Notifier:\n",
        "    @staticmethod\n",
        "    def notify(user, matching_jobs):\n",
        "        print(f\"\\nNotification for {user['name']} ({user['email']}):\")\n",
        "        print(f\"Found {len(matching_jobs)} new jobs matching your skills:\")\n",
        "        for job in matching_jobs:\n",
        "            print(f\"- {job['title']} at {job['company']}\")\n",
        "            print(f\"  Skills: {job['skills']}\")\n",
        "            print(f\"  Posted: {job['scraped_at']}\\n\")\n",
        "\n",
        "# Main Application\n",
        "class JobMonitor:\n",
        "    def __init__(self):\n",
        "        self.scraper = JobScraper()\n",
        "        self.clusterer = JobClusterer()\n",
        "        self.job_data = pd.DataFrame()\n",
        "\n",
        "        # Try to load existing model\n",
        "        if not self.clusterer.load_model():\n",
        "            print(\"No existing model found. Will train new model.\")\n",
        "\n",
        "    def run_daily_task(self):\n",
        "        print(f\"\\n{datetime.now()}: Running daily job monitoring...\")\n",
        "\n",
        "        # Step 1: Scrape new jobs\n",
        "        new_jobs = self.scraper.scrape_jobs()\n",
        "\n",
        "        if new_jobs.empty:\n",
        "            print(\"No new jobs scraped.\")\n",
        "            return\n",
        "\n",
        "        # Step 2: Load existing data\n",
        "        try:\n",
        "            existing_data = pd.read_csv(Config.DATA_FILE)\n",
        "            combined_data = pd.concat([existing_data, new_jobs]).drop_duplicates()\n",
        "        except:\n",
        "            combined_data = new_jobs\n",
        "\n",
        "        # Step 3: Save all data\n",
        "        combined_data.to_csv(Config.DATA_FILE, index=False)\n",
        "        self.job_data = combined_data\n",
        "\n",
        "        # Step 4: Train or update model\n",
        "        if self.clusterer.model is None:\n",
        "            print(\"Training new clustering model...\")\n",
        "            self.job_data = self.clusterer.train_model(self.job_data)\n",
        "            self.clusterer.save_model()\n",
        "\n",
        "        # Step 5: Check for user matches\n",
        "        self.check_user_matches(new_jobs)\n",
        "\n",
        "    def check_user_matches(self, new_jobs):\n",
        "        for user in Config.USER_PROFILES:\n",
        "            user_skills = TextPreprocessor.preprocess(user['skills'])\n",
        "            user_vector = self.clusterer.vectorizer.transform([user_skills])\n",
        "            user_cluster = self.clusterer.model.predict(user_vector)[0]\n",
        "\n",
        "            matching_jobs = []\n",
        "            for _, job in new_jobs.iterrows():\n",
        "                job_cluster, _ = self.clusterer.classify_job(job)\n",
        "                if job_cluster == user_cluster:\n",
        "                    matching_jobs.append(job)\n",
        "\n",
        "            if matching_jobs:\n",
        "                Notifier.notify(user, matching_jobs)\n",
        "\n",
        "# Run in Colab\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the monitor\n",
        "    monitor = JobMonitor()\n",
        "\n",
        "    # Run initial collection and processing\n",
        "    monitor.run_daily_task()\n",
        "\n",
        "    # Show sample of clustered jobs\n",
        "    if not monitor.job_data.empty and 'cluster' in monitor.job_data.columns:\n",
        "        print(\"\\nSample of clustered jobs:\")\n",
        "        print(monitor.job_data[['title', 'company', 'skills', 'cluster']].head())\n",
        "\n",
        "    # For production, you would add scheduling:\n",
        "    # schedule.every().day.at(\"09:00\").do(monitor.run_daily_task)\n",
        "    # while True:\n",
        "    #     schedule.run_pending()\n",
        "    #     time.sleep(60)"
      ]
    }
  ]
}